{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Claude 3.5 Sonnet: Interpreter Feedback Generation\n",
                "\n",
                "This notebook tests Claude 3.5 Sonnet's capabilities for generating detailed, actionable feedback for medical interpreters.\n",
                "\n",
                "## Objectives\n",
                "1. Test Claude's linguistic analysis capabilities\n",
                "2. Experiment with different prompt structures\n",
                "3. Generate detailed performance feedback\n",
                "4. Categorize feedback by skill area (grammar, terminology, completeness)\n",
                "5. Test few-shot learning for medical interpretation scenarios\n",
                "\n",
                "## Setup\n",
                "Requires Anthropic API key in `.env`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from anthropic import Anthropic\n",
                "import json\n",
                "import pandas as pd\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Initialize Anthropic client\n",
                "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
                "\n",
                "print(\"âœ… Claude client initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 1: Basic Feedback Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_feedback(source_text: str, interpretation: str, spacy_analysis: dict = None):\n",
                "    \"\"\"\n",
                "    Generate detailed interpreter feedback using Claude 3.5 Sonnet\n",
                "    \n",
                "    Args:\n",
                "        source_text: Original text to be interpreted\n",
                "        interpretation: Interpreter's rendition\n",
                "        spacy_analysis: Optional pre-computed spaCy analysis results\n",
                "    \n",
                "    Returns:\n",
                "        dict: Structured feedback with scores and recommendations\n",
                "    \"\"\"\n",
                "    \n",
                "    system_prompt = \"\"\"\n",
                "You are an expert medical interpreter trainer and assessor with deep knowledge of:\n",
                "- Medical terminology in English and Spanish\n",
                "- Linguistic accuracy and grammar\n",
                "- Interpretation ethics and standards\n",
                "- NBCMI and CCHI certification requirements\n",
                "\n",
                "Your task is to provide detailed, constructive feedback on interpreter performance.\n",
                "Focus on: accuracy, completeness, grammar, terminology, and professional standards.\n",
                "\"\"\"\n",
                "    \n",
                "    user_prompt = f\"\"\"\n",
                "Please analyze this interpretation and provide detailed feedback.\n",
                "\n",
                "SOURCE TEXT:\n",
                "{source_text}\n",
                "\n",
                "INTERPRETATION:\n",
                "{interpretation}\n",
                "\n",
                "Provide feedback in this JSON format:\n",
                "{{\n",
                "  \"overall_score\": <0-100>,\n",
                "  \"category_scores\": {{\n",
                "    \"accuracy\": <0-100>,\n",
                "    \"completeness\": <0-100>,\n",
                "    \"grammar\": <0-100>,\n",
                "    \"terminology\": <0-100>,\n",
                "    \"fluency\": <0-100>\n",
                "  }},\n",
                "  \"strengths\": [\"strength 1\", \"strength 2\", ...],\n",
                "  \"areas_for_improvement\": [\n",
                "    {{\n",
                "      \"category\": \"grammar|terminology|accuracy|completeness\",\n",
                "      \"issue\": \"description of the issue\",\n",
                "      \"example\": \"specific example from the interpretation\",\n",
                "      \"suggestion\": \"how to improve\",\n",
                "      \"severity\": \"critical|moderate|minor\"\n",
                "    }}\n",
                "  ],\n",
                "  \"key_omissions\": [\"omitted concept 1\", ...],\n",
                "  \"incorrect_additions\": [\"added concept 1\", ...],\n",
                "  \"summary\": \"overall performance summary\",\n",
                "  \"next_steps\": [\"action 1\", \"action 2\", ...]\n",
                "}}\n",
                "\"\"\"\n",
                "    \n",
                "    response = client.messages.create(\n",
                "        model=\"claude-3-5-sonnet-20241022\",\n",
                "        max_tokens=4096,\n",
                "        temperature=0.3,  # Lower temperature for more consistent analysis\n",
                "        system=system_prompt,\n",
                "        messages=[\n",
                "            {\"role\": \"user\", \"content\": user_prompt}\n",
                "        ]\n",
                "    )\n",
                "    \n",
                "    # Parse JSON response\n",
                "    feedback_text = response.content[0].text\n",
                "    \n",
                "    # Extract JSON from markdown code blocks if present\n",
                "    if \"```json\" in feedback_text:\n",
                "        feedback_text = feedback_text.split(\"```json\")[1].split(\"```\")[0]\n",
                "    elif \"```\" in feedback_text:\n",
                "        feedback_text = feedback_text.split(\"```\")[1].split(\"```\")[0]\n",
                "    \n",
                "    feedback = json.loads(feedback_text.strip())\n",
                "    \n",
                "    return feedback\n",
                "\n",
                "# Example\n",
                "# source = \"The patient has been experiencing severe headaches for the past three weeks.\"\n",
                "# interp = \"The patient have headaches for three weeks.\"\n",
                "# feedback = generate_feedback(source, interp)\n",
                "# print(json.dumps(feedback, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 2: Enhanced Feedback with NLP Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_enhanced_feedback(source: str, interpretation: str, nlp_data: dict):\n",
                "    \"\"\"\n",
                "    Generate feedback enhanced with spaCy/scispaCy analysis results\n",
                "    \n",
                "    Args:\n",
                "        source: Source text\n",
                "        interpretation: Interpretation text\n",
                "        nlp_data: Dictionary containing pre-computed NLP analysis:\n",
                "            - tense_accuracy: float\n",
                "            - medical_terminology_accuracy: float\n",
                "            - omitted_entities: list\n",
                "            - added_entities: list\n",
                "            - grammatical_errors: list\n",
                "    \"\"\"\n",
                "    \n",
                "    system_prompt = \"\"\"\n",
                "You are an expert medical interpreter trainer. You will receive:\n",
                "1. Source text and interpretation\n",
                "2. Automated NLP analysis results\n",
                "\n",
                "Your task is to synthesize this information into actionable, educational feedback.\n",
                "\"\"\"\n",
                "    \n",
                "    user_prompt = f\"\"\"\n",
                "SOURCE: {source}\n",
                "\n",
                "INTERPRETATION: {interpretation}\n",
                "\n",
                "AUTOMATED ANALYSIS:\n",
                "- Tense Accuracy: {nlp_data.get('tense_accuracy', 'N/A')}\n",
                "- Medical Terminology Accuracy: {nlp_data.get('medical_terminology_accuracy', 'N/A')}\n",
                "- Omitted Medical Entities: {nlp_data.get('omitted_entities', [])}\n",
                "- Added Entities: {nlp_data.get('added_entities', [])}\n",
                "- Grammatical Errors: {nlp_data.get('grammatical_errors', [])}\n",
                "\n",
                "Using this analysis, provide comprehensive feedback in JSON format (same structure as before).\n",
                "Explain WHY each issue matters in medical interpretation context.\n",
                "\"\"\"\n",
                "    \n",
                "    response = client.messages.create(\n",
                "        model=\"claude-3-5-sonnet-20241022\",\n",
                "        max_tokens=4096,\n",
                "        temperature=0.3,\n",
                "        system=system_prompt,\n",
                "        messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
                "    )\n",
                "    \n",
                "    feedback_text = response.content[0].text\n",
                "    if \"```json\" in feedback_text:\n",
                "        feedback_text = feedback_text.split(\"```json\")[1].split(\"```\")[0]\n",
                "    \n",
                "    return json.loads(feedback_text.strip())\n",
                "\n",
                "# Example usage combining with previous notebooks:\n",
                "# nlp_results = {\n",
                "#     'tense_accuracy': 0.67,\n",
                "#     'medical_terminology_accuracy': 0.80,\n",
                "#     'omitted_entities': ['severe', 'three weeks'],\n",
                "#     'added_entities': [],\n",
                "#     'grammatical_errors': ['subject-verb agreement: patient have']\n",
                "# }\n",
                "# feedback = generate_enhanced_feedback(source, interp, nlp_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 3: Prompt Engineering Experiments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different prompt variations\n",
                "prompts = {\n",
                "    \"direct\": \"Analyze this interpretation and provide feedback.\",\n",
                "    \n",
                "    \"role_play\": \"You are an NBCMI-certified medical interpreter trainer with 20 years of experience...\",\n",
                "    \n",
                "    \"structured\": \"\"\"Follow these steps:\n",
                "1. Compare source and interpretation for accuracy\n",
                "2. Identify omissions and additions\n",
                "3. Analyze grammar and syntax\n",
                "4. Assess medical terminology usage\n",
                "5. Provide actionable recommendations\"\"\",\n",
                "    \n",
                "    \"few_shot\": \"\"\"Here are examples of good feedback:\n",
                "\n",
                "Example 1:\n",
                "Source: \"The patient has diabetes.\"\n",
                "Interpretation: \"The patient have sugar.\"\n",
                "Feedback: \n",
                "- Grammar: Subject-verb agreement error (\"have\" should be \"has\")\n",
                "- Terminology: \"Sugar\" is colloquial; use \"diabetes\" for medical accuracy\n",
                "- Impact: Could cause confusion in medical records\n",
                "\n",
                "Now analyze this interpretation...\"\"\"\n",
                "}\n",
                "\n",
                "def test_prompt_variations(source: str, interpretation: str):\n",
                "    \"\"\"\n",
                "    Test different prompt engineering approaches\n",
                "    \"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    for prompt_type, prompt_template in prompts.items():\n",
                "        # Generate feedback with each prompt type\n",
                "        # Compare results, response time, token usage\n",
                "        pass\n",
                "    \n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 4: Learning Path Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_learning_path(feedback_history: list):\n",
                "    \"\"\"\n",
                "    Generate personalized learning path based on multiple assessments\n",
                "    \n",
                "    Args:\n",
                "        feedback_history: List of previous feedback objects\n",
                "    \n",
                "    Returns:\n",
                "        dict: Customized learning path with modules and exercises\n",
                "    \"\"\"\n",
                "    \n",
                "    system_prompt = \"\"\"\n",
                "You are an AI Mentor for medical interpreter training. \n",
                "Based on a student's assessment history, create a personalized learning path.\n",
                "\"\"\"\n",
                "    \n",
                "    # Aggregate weak areas from feedback history\n",
                "    weak_areas = []\n",
                "    for feedback in feedback_history:\n",
                "        for improvement in feedback.get('areas_for_improvement', []):\n",
                "            weak_areas.append(improvement['category'])\n",
                "    \n",
                "    # Count occurrences\n",
                "    from collections import Counter\n",
                "    area_counts = Counter(weak_areas)\n",
                "    \n",
                "    user_prompt = f\"\"\"\n",
                "The student has completed {len(feedback_history)} assessments.\n",
                "\n",
                "Recurring weak areas:\n",
                "{json.dumps(dict(area_counts), indent=2)}\n",
                "\n",
                "Recent feedback summary:\n",
                "{json.dumps(feedback_history[-3:], indent=2)}\n",
                "\n",
                "Create a personalized 4-week learning path in JSON format:\n",
                "{{\n",
                "  \"student_profile\": {{\n",
                "    \"current_level\": \"beginner|intermediate|advanced\",\n",
                "    \"priority_areas\": [\"area1\", \"area2\", ...]\n",
                "  }},\n",
                "  \"learning_path\": [\n",
                "    {{\n",
                "      \"week\": 1,\n",
                "      \"focus_area\": \"grammar|terminology|accuracy\",\n",
                "      \"modules\": [\n",
                "        {{\n",
                "          \"title\": \"module name\",\n",
                "          \"description\": \"what student will learn\",\n",
                "          \"estimated_time\": \"2 hours\",\n",
                "          \"exercises\": [\"exercise 1\", \"exercise 2\"]\n",
                "        }}\n",
                "      ]\n",
                "    }}\n",
                "  ]\n",
                "}}\n",
                "\"\"\"\n",
                "    \n",
                "    response = client.messages.create(\n",
                "        model=\"claude-3-5-sonnet-20241022\",\n",
                "        max_tokens=4096,\n",
                "        temperature=0.5,\n",
                "        system=system_prompt,\n",
                "        messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
                "    )\n",
                "    \n",
                "    learning_path_text = response.content[0].text\n",
                "    if \"```json\" in learning_path_text:\n",
                "        learning_path_text = learning_path_text.split(\"```json\")[1].split(\"```\")[0]\n",
                "    \n",
                "    return json.loads(learning_path_text.strip())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 5: Cost & Performance Benchmarking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_claude_performance(test_cases: list):\n",
                "    \"\"\"\n",
                "    Benchmark Claude API performance and costs\n",
                "    \n",
                "    Args:\n",
                "        test_cases: List of (source, interpretation) tuples\n",
                "    \"\"\"\n",
                "    import time\n",
                "    \n",
                "    results = []\n",
                "    \n",
                "    for source, interp in test_cases:\n",
                "        start_time = time.time()\n",
                "        \n",
                "        response = client.messages.create(\n",
                "            model=\"claude-3-5-sonnet-20241022\",\n",
                "            max_tokens=4096,\n",
                "            messages=[{\n",
                "                \"role\": \"user\",\n",
                "                \"content\": f\"Analyze this interpretation:\\nSource: {source}\\nInterpretation: {interp}\"\n",
                "            }]\n",
                "        )\n",
                "        \n",
                "        elapsed_time = time.time() - start_time\n",
                "        \n",
                "        # Extract usage statistics\n",
                "        usage = response.usage\n",
                "        \n",
                "        results.append({\n",
                "            'input_tokens': usage.input_tokens,\n",
                "            'output_tokens': usage.output_tokens,\n",
                "            'response_time': elapsed_time,\n",
                "            'estimated_cost': (usage.input_tokens * 0.003 / 1000) + (usage.output_tokens * 0.015 / 1000)\n",
                "        })\n",
                "    \n",
                "    df = pd.DataFrame(results)\n",
                "    \n",
                "    print(\"Performance Summary:\")\n",
                "    print(df.describe())\n",
                "    print(f\"\\nTotal Estimated Cost: ${df['estimated_cost'].sum():.4f}\")\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. Create comprehensive test dataset of medical interpretations\n",
                "2. Benchmark feedback quality across different prompt structures\n",
                "3. Optimize for cost vs quality trade-off\n",
                "4. Test Claude vs GPT-4 for feedback generation\n",
                "5. Implement caching strategy for similar interpretations\n",
                "6. Move successful patterns to `app/llm/claude_feedback.py`\n",
                "7. Begin integration with FastAPI endpoints"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}