{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# spaCy Grammar & Linguistic Analysis\n",
                "\n",
                "This notebook tests spaCy's capabilities for deep linguistic analysis of interpreter transcripts.\n",
                "\n",
                "## Objectives\n",
                "1. Test spaCy's grammar analysis (POS tagging, dependency parsing)\n",
                "2. Identify common grammatical errors in interpretations\n",
                "3. Test tense accuracy\n",
                "4. Analyze sentence structure and complexity\n",
                "5. Detect omissions and additions\n",
                "\n",
                "## Setup\n",
                "Requires spaCy models:\n",
                "- `en_core_web_lg` (English)\n",
                "- `es_core_news_lg` (Spanish)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import spacy\n",
                "from spacy import displacy\n",
                "import pandas as pd\n",
                "from collections import Counter\n",
                "\n",
                "# Download models if not already installed:\n",
                "# !python -m spacy download en_core_web_lg\n",
                "# !python -m spacy download es_core_news_lg\n",
                "\n",
                "# Load spaCy models\n",
                "nlp_en = spacy.load(\"en_core_web_lg\")\n",
                "nlp_es = spacy.load(\"es_core_news_lg\")\n",
                "\n",
                "print(\"âœ… spaCy models loaded\")\n",
                "print(f\"English model: {nlp_en.meta['name']} v{nlp_en.meta['version']}\")\n",
                "print(f\"Spanish model: {nlp_es.meta['name']} v{nlp_es.meta['version']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 1: Part-of-Speech (POS) Tagging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_pos(text: str, lang: str = 'en'):\n",
                "    \"\"\"\n",
                "    Analyze Part-of-Speech tags in text\n",
                "    \"\"\"\n",
                "    nlp = nlp_en if lang == 'en' else nlp_es\n",
                "    doc = nlp(text)\n",
                "    \n",
                "    pos_data = []\n",
                "    for token in doc:\n",
                "        pos_data.append({\n",
                "            'text': token.text,\n",
                "            'lemma': token.lemma_,\n",
                "            'pos': token.pos_,\n",
                "            'tag': token.tag_,\n",
                "            'dep': token.dep_,\n",
                "            'head': token.head.text\n",
                "        })\n",
                "    \n",
                "    return pd.DataFrame(pos_data)\n",
                "\n",
                "# Example\n",
                "sample_text = \"The patient has been experiencing severe headaches for the past three weeks.\"\n",
                "pos_df = analyze_pos(sample_text)\n",
                "print(pos_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 2: Dependency Parsing & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_dependencies(text: str, lang: str = 'en'):\n",
                "    \"\"\"\n",
                "    Visualize dependency parse tree\n",
                "    \"\"\"\n",
                "    nlp = nlp_en if lang == 'en' else nlp_es\n",
                "    doc = nlp(text)\n",
                "    \n",
                "    # Render in notebook\n",
                "    displacy.render(doc, style=\"dep\", jupyter=True)\n",
                "\n",
                "# Example\n",
                "# visualize_dependencies(\"The doctor prescribed antibiotics for the infection.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 3: Tense Detection & Accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_verb_tenses(text: str, lang: str = 'en'):\n",
                "    \"\"\"\n",
                "    Detect and categorize verb tenses\n",
                "    \"\"\"\n",
                "    nlp = nlp_en if lang == 'en' else nlp_es\n",
                "    doc = nlp(text)\n",
                "    \n",
                "    verbs = []\n",
                "    for token in doc:\n",
                "        if token.pos_ == 'VERB' or token.pos_ == 'AUX':\n",
                "            verbs.append({\n",
                "                'text': token.text,\n",
                "                'lemma': token.lemma_,\n",
                "                'tag': token.tag_,\n",
                "                'tense': token.morph.get('Tense'),\n",
                "                'aspect': token.morph.get('Aspect'),\n",
                "                'mood': token.morph.get('Mood')\n",
                "            })\n",
                "    \n",
                "    return pd.DataFrame(verbs)\n",
                "\n",
                "def compare_tense_accuracy(source_text: str, interpreted_text: str, lang: str = 'en'):\n",
                "    \"\"\"\n",
                "    Compare verb tenses between source and interpretation\n",
                "    \"\"\"\n",
                "    source_verbs = detect_verb_tenses(source_text, lang)\n",
                "    interp_verbs = detect_verb_tenses(interpreted_text, lang)\n",
                "    \n",
                "    print(\"Source Tenses:\")\n",
                "    print(source_verbs)\n",
                "    print(\"\\nInterpretation Tenses:\")\n",
                "    print(interp_verbs)\n",
                "    \n",
                "    # Basic tense accuracy\n",
                "    source_tenses = [t[0] if t else None for t in source_verbs['tense']]\n",
                "    interp_tenses = [t[0] if t else None for t in interp_verbs['tense']]\n",
                "    \n",
                "    matches = sum(1 for s, i in zip(source_tenses, interp_tenses) if s == i and s is not None)\n",
                "    total = min(len(source_tenses), len(interp_tenses))\n",
                "    \n",
                "    accuracy = matches / total if total > 0 else 0\n",
                "    print(f\"\\nTense Accuracy: {accuracy:.2%}\")\n",
                "    return accuracy\n",
                "\n",
                "# Example\n",
                "# source = \"I have been taking this medication for six months.\"\n",
                "# interpretation = \"I took this medication for six months.\"\n",
                "# compare_tense_accuracy(source, interpretation)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 4: Grammatical Error Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_common_errors(text: str, lang: str = 'en'):\n",
                "    \"\"\"\n",
                "    Detect common grammatical errors in medical interpretations\n",
                "    \n",
                "    Common errors to check:\n",
                "    - Subject-verb agreement\n",
                "    - Article usage (a/an/the)\n",
                "    - Preposition errors\n",
                "    - Pronoun reference clarity\n",
                "    \"\"\"\n",
                "    nlp = nlp_en if lang == 'en' else nlp_es\n",
                "    doc = nlp(text)\n",
                "    \n",
                "    errors = []\n",
                "    \n",
                "    # Check subject-verb agreement\n",
                "    for token in doc:\n",
                "        if token.dep_ == 'nsubj':\n",
                "            verb = token.head\n",
                "            # Basic agreement check\n",
                "            subj_number = token.morph.get('Number')\n",
                "            verb_number = verb.morph.get('Number')\n",
                "            \n",
                "            if subj_number and verb_number and subj_number != verb_number:\n",
                "                errors.append({\n",
                "                    'type': 'subject_verb_agreement',\n",
                "                    'subject': token.text,\n",
                "                    'verb': verb.text,\n",
                "                    'message': f\"Subject '{token.text}' ({subj_number}) doesn't agree with verb '{verb.text}' ({verb_number})\"\n",
                "                })\n",
                "    \n",
                "    return pd.DataFrame(errors) if errors else pd.DataFrame()\n",
                "\n",
                "# Example\n",
                "# text_with_errors = \"The patient have high blood pressure and they is taking medication.\"\n",
                "# errors = detect_common_errors(text_with_errors)\n",
                "# print(errors)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test 5: Semantic Similarity (Omissions & Additions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_content_alignment(source: str, interpretation: str, lang: str = 'en'):\n",
                "    \"\"\"\n",
                "    Analyze semantic alignment between source and interpretation\n",
                "    Identifies potential omissions or additions\n",
                "    \"\"\"\n",
                "    nlp = nlp_en if lang == 'en' else nlp_es\n",
                "    \n",
                "    doc_source = nlp(source)\n",
                "    doc_interp = nlp(interpretation)\n",
                "    \n",
                "    # Overall semantic similarity\n",
                "    similarity = doc_source.similarity(doc_interp)\n",
                "    \n",
                "    # Extract key entities and concepts\n",
                "    source_entities = {ent.text: ent.label_ for ent in doc_source.ents}\n",
                "    interp_entities = {ent.text: ent.label_ for ent in doc_interp.ents}\n",
                "    \n",
                "    # Find omissions (in source but not interpretation)\n",
                "    omissions = set(source_entities.keys()) - set(interp_entities.keys())\n",
                "    \n",
                "    # Find additions (in interpretation but not source)\n",
                "    additions = set(interp_entities.keys()) - set(source_entities.keys())\n",
                "    \n",
                "    print(f\"Semantic Similarity: {similarity:.2%}\")\n",
                "    print(f\"\\nPotential Omissions: {list(omissions)}\")\n",
                "    print(f\"Potential Additions: {list(additions)}\")\n",
                "    \n",
                "    return {\n",
                "        'similarity': similarity,\n",
                "        'omissions': list(omissions),\n",
                "        'additions': list(additions)\n",
                "    }\n",
                "\n",
                "# Example\n",
                "# source = \"The patient has diabetes and hypertension. He takes metformin daily.\"\n",
                "# interp = \"The patient has diabetes. He takes medication.\"\n",
                "# analyze_content_alignment(source, interp)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. Create test dataset of medical interpretation pairs (source + interpretation)\n",
                "2. Benchmark accuracy across different error types\n",
                "3. Fine-tune error detection rules for medical terminology\n",
                "4. Integrate with scispaCy for medical entity recognition (next notebook)\n",
                "5. Move successful patterns to `app/nlp/grammar_analyzer.py`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}